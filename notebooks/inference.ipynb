{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Recommendation Agent - ì¶”ë¡ (Inference) ë…¸íŠ¸ë¶\n",
    "\n",
    "**2025 DATAÂ·AI ë¶„ì„ ê²½ì§„ëŒ€íšŒ - ë…¼ë¬¸Â·ë°ì´í„° ì¶”ì²œ ì—ì´ì „íŠ¸**\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Qwen3-14B ê¸°ë°˜ ì—°êµ¬ ë°ì´í„°/ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì‹¤í–‰ í™˜ê²½\n",
    "- GPU: NVIDIA RTX 3080 ì´ìƒ (INT8 ì–‘ìí™” ì‹œ 14GB VRAM)\n",
    "- CUDA: 11.8+\n",
    "- Python: 3.10+\n",
    "\n",
    "## ì‹¤í–‰ ìˆœì„œ\n",
    "1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "2. ëª¨ë¸ ë¡œë“œ\n",
    "3. ì¶”ë¡  ì‹¤í–‰\n",
    "4. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}\")\n",
    "\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import asyncio\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(os.path.join(project_root, '.env'))\n",
    "print(\"âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU ë° CUDA í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPU í™•ì¸\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜: {torch.cuda.device_count()}\")\n",
    "    print(f\"í˜„ì¬ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œ ë˜ëŠ” DEV_MODEë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì¶”ì²œ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”\n",
    "\n",
    "**ì¤‘ìš”**: ì´ ë‹¨ê³„ì—ì„œ Qwen3-14B ëª¨ë¸ì´ ë¡œë“œë©ë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "- INT4: ~8GB VRAM\n",
    "- INT8: ~14GB VRAM\n",
    "- FP16: ~28GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ì²œ ì—ì´ì „íŠ¸ ì„í¬íŠ¸ ë° ì´ˆê¸°í™”\n",
    "from src.agents.recommendation_agent import KoreanResearchRecommendationAgent\n",
    "from src.config.settings import settings\n",
    "\n",
    "print(\"ëª¨ë¸ ì„¤ì •:\")\n",
    "print(f\"  - ëª¨ë¸ëª…: {settings.MODEL_NAME}\")\n",
    "print(f\"  - ì–‘ìí™”: {settings.QUANTIZATION}\")\n",
    "print(f\"  - ì„ë² ë”© ëª¨ë¸: {settings.EMBEDDING_MODEL}\")\n",
    "print(f\"  - ê°œë°œ ëª¨ë“œ: {settings.DEV_MODE}\")\n",
    "print(\"\\nğŸš€ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "\n",
    "agent = KoreanResearchRecommendationAgent()\n",
    "\n",
    "print(\"\\nâœ… ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"ëª¨ë¸ ì •ë³´: {json.dumps(agent.llm_model.get_model_info(), indent=2, ensure_ascii=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ID ì„¤ì •\n",
    "\n",
    "DataONì— ë“±ë¡ëœ ì‹¤ì œ ë°ì´í„°ì…‹ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ID\n",
    "# ì˜ˆì‹œ: KISTI DataONì˜ ì‹¤ì œ ë°ì´í„°ì…‹ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”\n",
    "test_dataset_id = \"SAMPLE_DATASET_ID\"  # TODO: ì‹¤ì œ ë°ì´í„°ì…‹ IDë¡œ ë³€ê²½\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ID: {test_dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì¶”ë¡  ì‹¤í–‰\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
    "1. ì†ŒìŠ¤ ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (DataON API)\n",
    "2. LLMìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n",
    "3. í›„ë³´ ìˆ˜ì§‘ (DataON + ScienceON API)\n",
    "4. í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê³„ì‚° (E5 + BM25)\n",
    "5. LLMìœ¼ë¡œ ìµœì¢… ì¶”ì²œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  ì‹¤í–‰ (ë¹„ë™ê¸°)\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"ğŸ” ì¶”ì²œ ì‹œì‘...\\n\")\n",
    "\n",
    "# Jupyterì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰\n",
    "result = await agent.recommend(test_dataset_id)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nâœ… ì¶”ì²œ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ë¥˜ í™•ì¸\n",
    "if 'error' in result:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {result['error']}\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“Š ì¶”ì²œ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nì†ŒìŠ¤ ë°ì´í„°ì…‹:\")\n",
    "    print(f\"  ID: {result['source_dataset']['id']}\")\n",
    "    print(f\"  ì œëª©: {result['source_dataset']['title']}\")\n",
    "    print(f\"  í‚¤ì›Œë“œ: {', '.join(result['source_dataset']['keywords'])}\")\n",
    "    \n",
    "    print(f\"\\nì¶”ì²œ ê°œìˆ˜: {len(result['recommendations'])}ê°œ\")\n",
    "    print(f\"ë¶„ì„ í›„ë³´: {result['candidates_analyzed']}ê°œ\")\n",
    "    print(f\"ì²˜ë¦¬ ì‹œê°„: {result['processing_time_ms']}ms\")\n",
    "    \n",
    "    print(f\"\\nëª¨ë¸ ì •ë³´:\")\n",
    "    for key, value in result['model_info'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“ ì¶”ì²œ ëª©ë¡\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ì²œ ëª©ë¡ ìƒì„¸ ì¶œë ¥\n",
    "if 'recommendations' in result:\n",
    "    for rec in result['recommendations']:\n",
    "        print(f\"\\n[{rec['rank']}ìœ„] {rec['level']} - {rec['type'].upper()}\")\n",
    "        print(f\"ì œëª©: {rec['title']}\")\n",
    "        print(f\"ì ìˆ˜: {rec['score']:.3f}\")\n",
    "        print(f\"ì´ìœ : {rec['reason']}\")\n",
    "        print(f\"URL: {rec['url']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. JSON íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "output_dir = os.path.join(project_root, 'data', 'inference_results')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_dir, f\"result_{test_dataset_id}.json\")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë¦¬ì†ŒìŠ¤ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if hasattr(agent, 'llm_model') and agent.llm_model:\n",
    "    agent.llm_model.cleanup()\n",
    "    print(\"âœ… ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì¶”ê°€ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)\n",
    "\n",
    "ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë°°ì¹˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ ë°ì´í„°ì…‹ ID ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
    "test_dataset_ids = [\n",
    "    \"DATASET_ID_1\",\n",
    "    \"DATASET_ID_2\",\n",
    "    \"DATASET_ID_3\"\n",
    "]\n",
    "\n",
    "batch_results = []\n",
    "\n",
    "for dataset_id in test_dataset_ids:\n",
    "    print(f\"\\nì²˜ë¦¬ ì¤‘: {dataset_id}\")\n",
    "    try:\n",
    "        result = await agent.recommend(dataset_id)\n",
    "        batch_results.append({\n",
    "            'dataset_id': dataset_id,\n",
    "            'success': 'error' not in result,\n",
    "            'result': result\n",
    "        })\n",
    "        print(f\"âœ… ì™„ë£Œ: {len(result.get('recommendations', []))}ê°œ ì¶”ì²œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì‹¤íŒ¨: {e}\")\n",
    "        batch_results.append({\n",
    "            'dataset_id': dataset_id,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# ë°°ì¹˜ ê²°ê³¼ ì €ì¥\n",
    "batch_output_file = os.path.join(output_dir, 'batch_results.json')\n",
    "with open(batch_output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(batch_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… ë°°ì¹˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {batch_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
