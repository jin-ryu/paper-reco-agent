# 문제해결 제안서

## 문제명
**2025 DATA·AI 분석 경진대회 - 논문·데이터 추천 에이전트**

## 팀명
[참가 팀명]

## 제안 제목
**하이브리드 검색 기반 연구 데이터 및 논문 추천 에이전트**

---

## 제안 내용

### 1. 데이터 활용 방안 및 분석 방법론

#### 1.1 배경 및 목표
본 제안은 KISTI DataON에 등록된 방대한 연구데이터의 활용성을 극대화하는 것을 목표로 합니다. 연구자가 특정 데이터셋을 조회할 때, 그 데이터의 핵심 의도를 정확히 파악하여 가장 연관성 높은 국내외 논문과 후속 연구 데이터셋을 자동으로 추천해주는 LLM 기반 에이전트를 개발하고자 합니다.

이를 위해, 저희는 먼저 다양한 소형언어모델(SLM)이 다국어 연구데이터의 메타데이터로부터 얼마나 정확하고 빠르게 핵심 키워드를 추출하는지를 체계적으로 평가했습니다. 이 평가는 실제 서비스 환경을 고려하여 속도, 안정성, 정확성, 다국어 처리 능력 등 다각적인 지표를 기준으로 진행되었습니다.

#### 1.2 데이터 활용 방안
- **입력 데이터**: KISTI DataON의 연구데이터 메타데이터 (제목, 설명, 키워드)
- **활용 API**:
  - DataON 검색/상세 API: 관련 데이터셋 검색 및 메타데이터 수집
  - ScienceON 논문 검색 API: 관련 논문 검색 및 메타데이터 수집
- **활용 모델**:
  - Google Gemma-2-9B-IT (기본 모델)
  - Alibaba Qwen3-14B (대안 모델)
  - Multilingual E5 임베딩 모델 (의미적 유사도 계산)
- **생성 데이터**:
  - LLM이 추출한 '핵심 키워드'
  - 이를 바탕으로 추천된 '연관 논문/데이터셋 목록'
  - 논리적이고 구체적인 추천 근거 (자연어 생성)

#### 1.3 분석 방법론: '최적 키워드 추출'을 위한 3단계 분석

추천 품질은 **'입력 데이터의 핵심을 얼마나 잘 이해하는가'** 에 달려있다고 판단했습니다. 이를 위해 아래와 같은 3단계 분석 방법론을 설계하고 실행했습니다.

##### 1단계: 평가 인프라 구축
- **다국어 테스트셋 구성**: DataON API를 통해 한국어, 영어, 일본어 및 다국어가 혼합된 13개의 실제 연구데이터셋을 수집하여 평가 데이터의 다양성과 현실성을 확보했습니다 (`data/test/testset_aug.json`).
- **평가 환경 표준화**: NVIDIA H100 GPU 환경에서 모든 모델이 동일한 조건(FP16, CUDA 12.8)에서 평가되도록 통제하여 공정한 성능 비교가 가능하도록 했습니다.
- **Ground Truth 구축**:
  - 각 테스트 케이스에 대해 관련 논문 및 데이터셋 후보군을 수작업으로 검증하여 신뢰할 수 있는 평가 기준을 마련했습니다.
  - `testset_aug.json` 내 `candidate_pool` 구조:
    - `papers`: 각 테스트 케이스별 관련 논문 후보 5개 (relevance_score: 0/1/2로 관련성 표시)
    - `datasets`: 각 테스트 케이스별 관련 데이터셋 후보 5개 (relevance_score: 0/1/2로 관련성 표시)

##### 2단계: 정량적 성능 평가 (속도 및 안정성)
- 모든 모델을 대상으로 13개 데이터셋에 대한 키워드 추출 및 추천 자동화 테스트를 수행하여 평균 응답 시간, 성공률, 에러 발생률을 측정했습니다.
- **[결과]** Google의 Gemma-2-9B-IT 모델이 평균 29.5초의 안정적인 응답 시간과 100%의 성공률(13/13)을 기록하며 실시간 서비스에 가장 적합함을 확인했습니다.

##### 3단계: 정성적 성능 평가 (정확성 및 다국어 처리 능력)
- 각 모델이 추출한 키워드를 원본 데이터의 내용과 비교하여 핵심 개념 포함 여부, 전문 용어 추출 정확도, 불필요한 번역 여부 등을 심층 분석했습니다.
- **[결과]** Gemma-2-9B-IT는 한국어 전문 용어(예: GeoAI, COVID-19, 심전도)를 정확히 인식했으며, 해외 데이터셋에서 nDCG@5 1.0의 완벽한 성능을 달성하여 원본 언어를 훼손하지 않고 키워드를 추출함을 입증했습니다.

#### 1.4 하이브리드 검색 기반 추천 파이프라인

##### 1단계: 지능형 검색 쿼리 생성
- LLM(Gemma-2-9B-IT)을 활용하여 입력 데이터셋을 의미적으로 분석
- 논문 검색용/데이터셋 검색용 최적화된 쿼리를 각각 3-5개씩 생성
- 키워드 전처리를 통한 특수문자 제거 및 중복 제거

##### 2단계: 하이브리드 후보 수집 및 스코어링
- **의미적 유사도 (Semantic)**: Multilingual E5 임베딩 기반 코사인 유사도 계산
- **어휘적 유사도 (Lexical)**: BM25 알고리즘 기반 키워드 매칭 점수 계산
- **하이브리드 가중치**:
  - 논문: Semantic 80% + Lexical 20% (의미 중심)
  - 데이터셋: Semantic 60% + Lexical 40% (키워드 중심)
- 키워드별 병렬 검색 후 중복 제거 및 상위 10개 후보 선별

##### 3단계: LLM 기반 최종 추천 및 순위 결정
- 상위 10개 후보에 대한 정밀 분석
- 유사도 점수 해석 및 추천 순위 결정
- **신뢰도 높은 추천 근거 생성**: "유사도가 높기 때문"이라는 단순한 설명을 넘어, "입력 데이터의 'A' 개념과 후보 논문의 'B' 연구 방법론이 직접적으로 연관되어 있음"과 같이 논리적이고 구체적인 추천 사유를 자연어로 생성
- 추천 수준 분류 (강추/추천/참고)

#### 1.5 성능 최적화 전략
- **경량화**: FP16 정밀도로 메모리 사용량 절반 감소 (~18GB for Gemma)
- **안정성**: Description 길이 제한(1000자), 재시도 로직(최대 2회), 키워드 전처리
- **속도**: 병렬 API 호출, 임베딩 사전 계산으로 평균 29.5초 응답

---

### 2. 추진일정(계획)

| 기간 | 주요 내용 | 세부 활동                                                                                                                                                                                                                                         |
|------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **1주차** | 프로젝트 기획 및 기술 검증 | - 요구사항 분석 및 목표 설정<br>- DataON, ScienceON API 기능 명세 분석<br>- 최적 LLM 선정을 위한 성능 평가 완료 (Gemma-2-9B 선정)<br>- 개발 환경 설정 (CUDA, Python, 의존성 설치)                                                                                                        |
| **2주차** | 핵심 추천 모듈 개발 | - Gemma-2-9B-IT를 활용한 키워드 추출 기능 구현<br>- API 호출을 통한 후보군(논문/데이터) 검색 기능 구현<br>- Multilingual E5 임베딩 모델을 활용한 유사도 계산 기능 개발<br>- BM25 어휘 매칭 점수 계산 기능 개발<br>- 하이브리드 검색 기반 추천 파이프라인 구현                                                                 |
| **3주차** | 에이전트 고도화 및 평가 | - LLM을 '평가자'로 활용하여 추천 결과의 relevance_score 자동 부여 기능 개발<br>- 자체 구축한 테스트셋(testset_aug.json) 기반 성능 평가 (nDCG, MRR, Recall, Precision 지표 활용)<br>- Qwen3-14B를 이용한 이중 검증(Cross-check) 로직 추가<br>- 하이브리드 가중치 튜닝 (α, β 조정)<br>- 응답 시간 최적화 (병렬화) 및 안정성 개선 |
| **4주차** | 최종 결과물 제출 준비 | - .ipynb 노트북 환경에 전체 실행 코드 및 설명서 정리<br>- README 및 설명 문서 작성<br>- 데모 영상 제작<br>- 문제 해결 제안서 및 최종 결과 보고서 작성<br>- 제출물 최종 검토 및 제출                                                                                                                     |

---

### 3. 예상 결과물

#### 3.1 LLM 기반 추천 에이전트의 핵심 기능

##### 3.1.1 다국어 의미 분석
- 입력 데이터의 언어에 상관없이 핵심 의미를 파악하고 최적의 검색 키워드를 자동 생성하는 기능
- 한국어, 영어, 일본어 등 다국어 데이터 처리 능력 (Multilingual E5 임베딩)
- 해외 데이터셋에서 nDCG@5 1.0의 완벽한 성능 달성

##### 3.1.2 하이브리드 랭킹
- **1차 랭킹**: 의미 기반 유사도(코사인 유사도) + 어휘 매칭(BM25)로 후보군을 정렬
- **2차 랭킹**: LLM의 종합적 판단을 통해 최종 추천 순위와 근거를 생성하는 하이브리드 추천 로직
- 논문/데이터셋별로 최적화된 가중치 적용 (논문: 의미 80%, 데이터셋: 의미 60%)

##### 3.1.3 신뢰도 높은 추천 근거
- "유사도가 높기 때문"이라는 단순한 설명을 넘어, **"입력 데이터의 'A' 개념과 후보 논문의 'B' 연구 방법론이 직접적으로 연관되어 있음"** 과 같이 논리적이고 구체적인 추천 사유를 자연어로 생성
- 추천 수준 분류 (강추/추천/참고)로 신뢰도를 명확히 표현

#### 3.2 제출물 구성

- **README.md**: 연구결과물(데이터 및 모델, 학습/추론 수행방법, 실행환경 등)을 설명하는 Markdown 형식의 파일
- **data**: 모델의 평가에 사용된 데이터 파일을 담고 있는 폴더
  - `test/testset_aug.json`: 13개 테스트 케이스 및 Ground Truth (candidate_pool)
- **model**: 학습된 모델 파일들을 담고 있는 폴더 (자동 다운로드됨)
  - `google/gemma-2-9b-it`: 기본 LLM 모델
  - `intfloat/multilingual-e5-large`: 임베딩 모델
- **src**: 데이터 정제/전처리, 모델 구조, 모델의 학습과 평가를 수행하는 소스코드 파일들(.py)을 담고 있는 폴더
  - `agents/`: 추천 에이전트 (핵심 추천 로직)
  - `clients/`: API 클라이언트 (DataON, ScienceON)
  - `models/`: LLM 모델 래퍼 및 프롬프트 템플릿
  - `tools/`: 검색, 임베딩, 유사도 계산 도구
  - `evaluation/`: 평가 메트릭 (nDCG, MRR, Recall, Precision)
- **notebooks**: 데이터 정제/전처리, 모델 구조, 모델의 학습과 평가를 수행하는 notebook 파일들(.ipynb)을 담고 있는 폴더
  - `inference.ipynb`: 추론 실행 (데이터셋 ID 입력 → 추천 결과 생성)
  - `evaluation.ipynb`: 성능 평가 (테스트셋 기반 평가 지표 계산)
- **scripts**: 환경 설정, 데이터 정제/전처리, 모델의 학습 및 평가 등을 수행하기 위한 스크립트 파일들을 담고 있는 폴더
  - `setup_environment.sh`: 자동 환경 설정 스크립트
- **demo**: 테스트 실행 데모 영상
- **results**: 모델의 추론 결과 및 평가 결과를 담고 있는 폴더
  - `inference_results/`: 추론 결과 (JSON 형식)
  - `evaluation_results/`: 평가 결과 (EVALUATION_SUMMARY.txt, metrics.json, detailed_results.csv 등)

#### 3.3 자체 성능 검증 보고서

본 프로젝트의 성능을 객관적으로 입증하기 위해 자체적으로 구축한 평가 시스템을 제공합니다.

##### 3.3.1 평가 데이터셋 (`data/test/testset_aug.json`)
- **구성**: 13개의 다국어 실제 연구데이터셋 (국내 11개, 해외 2개)
- **테스트 케이스 구조**:
  - `input_dataset`: 입력 데이터셋 정보 (svc_id, title, description, keywords)
  - `candidate_pool`: 각 테스트 케이스별 Ground Truth
    - `papers`: 관련 논문 후보 5개 (relevance_score: 0/1/2로 관련성 표시)
    - `datasets`: 관련 데이터셋 후보 5개 (relevance_score: 0/1/2로 관련성 표시)

##### 3.3.2 평가 지표 정의
- **nDCG@k (Normalized Discounted Cumulative Gain)**: 순위 품질을 측정하는 지표 (0~1). 1.0에 가까울수록 이상적인 순위.
- **MRR@k (Mean Reciprocal Rank)**: 첫 번째 관련 아이템의 역순위 평균 (0~1). 1.0은 첫 번째 추천이 관련 있음을 의미.
- **Recall@k**: 전체 관련 항목 중 상위 k개 추천에 포함된 비율 (0~1). 재현율.
- **Precision@k**: 상위 k개 추천 중 실제 관련 있는 항목의 비율 (0~1). 정밀도.

위 지표들을 k=3, k=5 두 가지 경우에 대해 계산하여 추천 시스템의 성능을 다각도로 평가합니다.

##### 3.3.3 평가 환경
- **평가 일시**: 2025-10-16 08:15
- **LLM 모델**: google/gemma-2-9b-it (9B 파라미터)
- **임베딩 모델**: intfloat/multilingual-e5-large
- **테스트 케이스**: 13개 (성공률 100%, 13/13)
- **평균 응답 시간**: 29.5초 (최소 20.1초 ~ 최대 37.5초)

##### 3.3.4 평가 결과 (k=5 기준)
위 성능 지표는 다음 파일에 저장되어 있습니다:
- `results/evaluation_results/202510160815/EVALUATION_SUMMARY.txt`: 평가 요약 리포트
- `results/evaluation_results/202510160815/detailed_results.csv`: 케이스별 상세 결과
- `results/evaluation_results/202510160815/metrics.json`: 평가 지표 전체 데이터 (k=3, k=5 모두 포함)
- `results/evaluation_results/202510160815/recommend_result.json`: 추천 결과 원본 데이터

**전체 추천 품질**

| 메트릭 | 점수 | 설명 |
| --- | --- | --- |
| **nDCG@5** | **0.8085** (±0.3634) | 순위 품질 - 이상적 순위 대비 80.85% |
| **MRR@5** | **0.8462** (±0.3755) | 첫 관련 아이템 순위 - 평균 1.18위 |
| **Recall@5** | **0.3669** (±0.1914) | 재현율 - 전체 관련 항목의 36.69% 발견 |
| **Precision@5** | **0.5692** (±0.3250) | 정밀도 - 추천 중 56.92%가 관련 있음 |

**타입별 성능**

| 타입 | nDCG@5 | MRR@5 | Recall@5 | Precision@5 |
| --- | --- | --- | --- | --- |
| 논문 | 0.8327 (±0.3708) | 0.8462 (±0.3755) | 0.6564 (±0.4034) | 0.5423 (±0.3396) |
| **데이터셋** | **0.9199** (±0.2765) | **0.9231** (±0.2774) | **0.7474** (±0.3149) | **0.6385** (±0.3176) |

**카테고리별 성능**

| 카테고리 | 전체 | 논문 | 데이터셋 | 테스트 케이스 수 |
| --- | --- | --- | --- | --- |
| 국내 데이터셋 | 0.7736 | 0.8023 | 0.9071 | 11개 |
| 해외 데이터셋 | **1.0000** | **1.0000** | 0.9908 | 2개 |

**시스템 성능**

| 항목 | 측정값 |
| --- | --- |
| 평균 응답 시간 | 29.5초 (최소 20.1초 ~ 최대 37.5초) |
| 메모리 사용량 | ~18GB (Gemma-2-9B-IT, FP16) |
| 처리 성공률 | 100% (13/13) |
| 실패율 | 0% |

#### 3.4 주요 성과
- **데이터셋 추천 우수**: nDCG 0.92, MRR 0.92, Recall 0.75로 특히 뛰어난 성능
- **안정적 동작**: 13개 테스트 케이스 100% 성공
- **해외 데이터 완벽 대응**: 해외 데이터셋에서 nDCG 1.0 달성
- **높은 순위 품질**: 전체 nDCG 0.81, MRR 0.85로 우수한 추천 순위

### 4. 기대효과

- **연구 효율성 향상**: 관련 논문 및 데이터셋을 빠르게 발견하여 연구 시간 단축
- **의미 기반 추천**: 키워드 검색의 한계를 극복하고 의미적으로 연관된 자료 추천
- **다국어 지원**: 글로벌 연구 데이터 활용성 증대 (국내 0.77, 해외 1.0)
- **비용 효율성**: 소규모 LLM(9B) 활용으로 합리적인 비용으로 고품질 추천 가능
- **데이터 발견 강화**: 특히 데이터셋 간 연관성 파악에 강점 (Recall 74.74%)
- **신뢰도 제공**: 구체적이고 논리적인 추천 근거 제시로 연구자의 의사결정 지원