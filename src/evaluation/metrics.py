"""
ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ë©”íŠ¸ë¦­

- nDCG@k: Normalized Discounted Cumulative Gain
- MRR@k: Mean Reciprocal Rank
- Recall@k: ì¬í˜„ìœ¨
- Precision@k: ì •ë°€ë„
"""

import numpy as np
from typing import List, Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)


def calculate_dcg(relevance_scores: List[float], k: int = 10) -> float:
    """
    DCG (Discounted Cumulative Gain) ê³„ì‚°

    Args:
        relevance_scores: ìˆœìœ„ë³„ ê´€ë ¨ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ kê°œê¹Œì§€ë§Œ ê³ ë ¤

    Returns:
        DCG ì ìˆ˜
    """
    try:
        relevance_scores = relevance_scores[:k]

        if not relevance_scores:
            return 0.0

        # DCG = sum(rel_i / log2(i+1))
        dcg = 0.0
        for i, rel in enumerate(relevance_scores, 1):
            dcg += rel / np.log2(i + 1)

        return dcg
    except Exception as e:
        logger.error(f"DCG ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0


def calculate_ndcg(relevance_scores: List[float], k: int = 10) -> float:
    """
    nDCG@k (Normalized Discounted Cumulative Gain) ê³„ì‚°

    ì¶”ì²œ ì‹œìŠ¤í…œì˜ ìˆœìœ„ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì§€í‘œ
    - 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì´ìƒì ì¸ ìˆœìœ„
    - 0.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì§€ ì•Šì€ ìˆœìœ„

    Args:
        relevance_scores: ìˆœìœ„ë³„ ê´€ë ¨ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [3, 2, 3, 0, 1, 2])
        k: ìƒìœ„ kê°œê¹Œì§€ë§Œ ê³ ë ¤ (default: 10)

    Returns:
        nDCG@k ì ìˆ˜ (0.0 ~ 1.0)
    """
    try:
        if not relevance_scores:
            return 0.0

        # DCG ê³„ì‚°
        dcg = calculate_dcg(relevance_scores, k)

        # IDCG (Ideal DCG) ê³„ì‚°: ì´ìƒì ì¸ ìˆœìœ„ (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
        ideal_relevance = sorted(relevance_scores, reverse=True)
        idcg = calculate_dcg(ideal_relevance, k)

        # nDCG = DCG / IDCG
        if idcg == 0:
            return 0.0

        ndcg = dcg / idcg
        return ndcg

    except Exception as e:
        logger.error(f"nDCG ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0


def calculate_mrr(relevance_scores: List[float], k: int = 10) -> float:
    """
    MRR@k (Mean Reciprocal Rank) ê³„ì‚°

    ì²« ë²ˆì§¸ ê´€ë ¨ ìˆëŠ” ì•„ì´í…œì˜ ìˆœìœ„ì˜ ì—­ìˆ˜
    - 1.0: ì²« ë²ˆì§¸ ì¶”ì²œì´ ê´€ë ¨ ìˆìŒ
    - 0.5: ë‘ ë²ˆì§¸ ì¶”ì²œì´ ê´€ë ¨ ìˆìŒ
    - 0.0: kê°œ ì•ˆì— ê´€ë ¨ ìˆëŠ” ì¶”ì²œ ì—†ìŒ

    Args:
        relevance_scores: ìˆœìœ„ë³„ ê´€ë ¨ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ kê°œê¹Œì§€ë§Œ ê³ ë ¤

    Returns:
        MRR@k ì ìˆ˜ (0.0 ~ 1.0)
    """
    try:
        relevance_scores = relevance_scores[:k]

        if not relevance_scores:
            return 0.0

        # ì²« ë²ˆì§¸ ê´€ë ¨ ìˆëŠ” ì•„ì´í…œì˜ ìˆœìœ„ ì°¾ê¸° (relevance > 0)
        for i, rel in enumerate(relevance_scores, 1):
            if rel > 0:
                return 1.0 / i

        return 0.0

    except Exception as e:
        logger.error(f"MRR ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0


def calculate_recall_at_k(
    recommended_ids: List[str],
    relevant_ids: List[str],
    k: int = 5
) -> float:
    """
    Recall@k ê³„ì‚°

    ì „ì²´ ê´€ë ¨ ìˆëŠ” ì•„ì´í…œ ì¤‘ ìƒìœ„ kê°œ ì¶”ì²œì— í¬í•¨ëœ ë¹„ìœ¨

    Args:
        recommended_ids: ì¶”ì²œëœ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        relevant_ids: ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ kê°œê¹Œì§€ë§Œ ê³ ë ¤

    Returns:
        Recall@k (0.0 ~ 1.0)
    """
    try:
        if not relevant_ids:
            return 0.0

        recommended_set = set(recommended_ids[:k])
        relevant_set = set(relevant_ids)

        # êµì§‘í•©ì˜ í¬ê¸° / ì „ì²´ ê´€ë ¨ ì•„ì´í…œ ìˆ˜
        intersection = recommended_set.intersection(relevant_set)
        recall = len(intersection) / len(relevant_set)

        return recall

    except Exception as e:
        logger.error(f"Recall@k ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0


def calculate_precision_at_k(
    recommended_ids: List[str],
    relevant_ids: List[str],
    k: int = 5
) -> float:
    """
    Precision@k ê³„ì‚°

    ìƒìœ„ kê°œ ì¶”ì²œ ì¤‘ ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ì•„ì´í…œì˜ ë¹„ìœ¨

    Args:
        recommended_ids: ì¶”ì²œëœ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        relevant_ids: ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ kê°œê¹Œì§€ë§Œ ê³ ë ¤

    Returns:
        Precision@k (0.0 ~ 1.0)
    """
    try:
        if not recommended_ids:
            return 0.0

        recommended_set = set(recommended_ids[:k])
        relevant_set = set(relevant_ids)

        # êµì§‘í•©ì˜ í¬ê¸° / ì¶”ì²œëœ ì•„ì´í…œ ìˆ˜
        intersection = recommended_set.intersection(relevant_set)
        precision = len(intersection) / min(len(recommended_set), k)

        return precision

    except Exception as e:
        logger.error(f"Precision@k ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0


def evaluate_recommendations(
    predictions: List[Dict[str, Any]],
    ground_truth: Dict[str, Any],
    k_values: List[int] = [3, 5, 10]
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¶”ì²œ ê²°ê³¼ í‰ê°€

    Args:
        predictions: ëª¨ë¸ì´ ìƒì„±í•œ ì¶”ì²œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            [{"type": "paper", "id": "...", "rank": 1, "score": 0.9}, ...]
        ground_truth: ì •ë‹µ ë°ì´í„° (candidate_pool í¬í•¨)
            {"papers": [...], "datasets": [...]}
        k_values: í‰ê°€í•  k ê°’ë“¤

    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    try:
        results = {}

        # ì¶”ì²œëœ IDì™€ íƒ€ì… ì¶”ì¶œ
        predicted_papers = [p for p in predictions if p.get('type') == 'paper']
        predicted_datasets = [p for p in predictions if p.get('type') == 'dataset']

        predicted_paper_ids = [p.get('id', '') for p in predicted_papers]
        predicted_dataset_ids = [p.get('id', '') for p in predicted_datasets]

        # Ground truthì—ì„œ ê´€ë ¨ì„± ì ìˆ˜ ë§¤í•‘
        paper_relevance_map = {}
        dataset_relevance_map = {}

        if 'papers' in ground_truth:
            for paper in ground_truth['papers']:
                paper_id = paper.get('id', '')
                relevance = paper.get('relevance_score', 0)
                paper_relevance_map[paper_id] = relevance

        if 'datasets' in ground_truth:
            for dataset in ground_truth['datasets']:
                dataset_id = dataset.get('id', '')
                relevance = dataset.get('relevance_score', 0)
                dataset_relevance_map[dataset_id] = relevance

        # ê´€ë ¨ ìˆëŠ” ì•„ì´í…œ í•„í„°ë§ (relevance_score > 0)
        relevant_paper_ids = [pid for pid, rel in paper_relevance_map.items() if rel > 0]
        relevant_dataset_ids = [did for did, rel in dataset_relevance_map.items() if rel > 0]

        # ê° k ê°’ì— ëŒ€í•´ í‰ê°€
        for k in k_values:
            # ë…¼ë¬¸ í‰ê°€
            paper_relevance_scores = [
                paper_relevance_map.get(pid, 0) for pid in predicted_paper_ids[:k]
            ]

            results[f'paper_ndcg@{k}'] = calculate_ndcg(paper_relevance_scores, k)
            results[f'paper_mrr@{k}'] = calculate_mrr(paper_relevance_scores, k)
            results[f'paper_recall@{k}'] = calculate_recall_at_k(
                predicted_paper_ids, relevant_paper_ids, k
            )
            results[f'paper_precision@{k}'] = calculate_precision_at_k(
                predicted_paper_ids, relevant_paper_ids, k
            )

            # ë°ì´í„°ì…‹ í‰ê°€
            dataset_relevance_scores = [
                dataset_relevance_map.get(did, 0) for did in predicted_dataset_ids[:k]
            ]

            results[f'dataset_ndcg@{k}'] = calculate_ndcg(dataset_relevance_scores, k)
            results[f'dataset_mrr@{k}'] = calculate_mrr(dataset_relevance_scores, k)
            results[f'dataset_recall@{k}'] = calculate_recall_at_k(
                predicted_dataset_ids, relevant_dataset_ids, k
            )
            results[f'dataset_precision@{k}'] = calculate_precision_at_k(
                predicted_dataset_ids, relevant_dataset_ids, k
            )

            # ì „ì²´ í‰ê°€ (ë…¼ë¬¸ + ë°ì´í„°ì…‹)
            all_predicted_ids = predicted_paper_ids + predicted_dataset_ids
            all_relevant_ids = relevant_paper_ids + relevant_dataset_ids
            all_relevance_map = {**paper_relevance_map, **dataset_relevance_map}

            all_relevance_scores = [
                all_relevance_map.get(item_id, 0) for item_id in all_predicted_ids[:k]
            ]

            results[f'overall_ndcg@{k}'] = calculate_ndcg(all_relevance_scores, k)
            results[f'overall_mrr@{k}'] = calculate_mrr(all_relevance_scores, k)
            results[f'overall_recall@{k}'] = calculate_recall_at_k(
                all_predicted_ids, all_relevant_ids, k
            )
            results[f'overall_precision@{k}'] = calculate_precision_at_k(
                all_predicted_ids, all_relevant_ids, k
            )

        # í†µê³„ ì •ë³´ ì¶”ê°€
        results['num_predicted_papers'] = len(predicted_papers)
        results['num_predicted_datasets'] = len(predicted_datasets)
        results['num_relevant_papers'] = len(relevant_paper_ids)
        results['num_relevant_datasets'] = len(relevant_dataset_ids)

        return results

    except Exception as e:
        logger.error(f"ì¶”ì²œ í‰ê°€ ì‹¤íŒ¨: {e}")
        return {}


def batch_evaluate(
    all_predictions: List[Dict[str, Any]],
    all_ground_truths: List[Dict[str, Any]],
    k_values: List[int] = [3, 5, 10]
) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë°°ì¹˜ í‰ê°€

    Args:
        all_predictions: ëª¨ë“  ë°ì´í„°ì…‹ì˜ ì¶”ì²œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        all_ground_truths: ëª¨ë“  ë°ì´í„°ì…‹ì˜ ì •ë‹µ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        k_values: í‰ê°€í•  k ê°’ë“¤

    Returns:
        í‰ê·  ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    try:
        if len(all_predictions) != len(all_ground_truths):
            raise ValueError("ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤")

        all_results = []

        for pred, gt in zip(all_predictions, all_ground_truths):
            result = evaluate_recommendations(pred, gt, k_values)
            all_results.append(result)

        # í‰ê·  ê³„ì‚°
        avg_results = {}

        # ëª¨ë“  ë©”íŠ¸ë¦­ í‚¤ ìˆ˜ì§‘
        all_keys = set()
        for result in all_results:
            all_keys.update(result.keys())

        # ìˆ«ì ë©”íŠ¸ë¦­ë§Œ í‰ê·  ê³„ì‚°
        numeric_keys = [k for k in all_keys if not k.startswith('num_')]

        for key in numeric_keys:
            values = [r.get(key, 0) for r in all_results]
            avg_results[f'avg_{key}'] = np.mean(values)
            avg_results[f'std_{key}'] = np.std(values)

        # í†µê³„ ì •ë³´
        avg_results['total_evaluated'] = len(all_results)
        avg_results['individual_results'] = all_results

        return avg_results

    except Exception as e:
        logger.error(f"ë°°ì¹˜ í‰ê°€ ì‹¤íŒ¨: {e}")
        return {}


def format_evaluation_report(eval_results: Dict[str, Any], k_values: List[int] = [3, 5, 10]) -> str:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…

    Args:
        eval_results: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        k_values: í‰ê°€í•œ k ê°’ë“¤

    Returns:
        í¬ë§·íŒ…ëœ ë¦¬í¬íŠ¸ ë¬¸ìì—´
    """
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ê²°ê³¼")
    report_lines.append("=" * 80)
    report_lines.append("")

    for k in k_values:
        report_lines.append(f"ğŸ“Š k={k} í‰ê°€ ê²°ê³¼")
        report_lines.append("-" * 80)

        # Overall ë©”íŠ¸ë¦­
        report_lines.append(f"  ì „ì²´ (Overall):")
        report_lines.append(f"    - nDCG@{k}:     {eval_results.get(f'overall_ndcg@{k}', 0):.4f}")
        report_lines.append(f"    - MRR@{k}:      {eval_results.get(f'overall_mrr@{k}', 0):.4f}")
        report_lines.append(f"    - Recall@{k}:   {eval_results.get(f'overall_recall@{k}', 0):.4f}")
        report_lines.append(f"    - Precision@{k}: {eval_results.get(f'overall_precision@{k}', 0):.4f}")
        report_lines.append("")

        # Paper ë©”íŠ¸ë¦­
        report_lines.append(f"  ë…¼ë¬¸ (Papers):")
        report_lines.append(f"    - nDCG@{k}:     {eval_results.get(f'paper_ndcg@{k}', 0):.4f}")
        report_lines.append(f"    - MRR@{k}:      {eval_results.get(f'paper_mrr@{k}', 0):.4f}")
        report_lines.append(f"    - Recall@{k}:   {eval_results.get(f'paper_recall@{k}', 0):.4f}")
        report_lines.append(f"    - Precision@{k}: {eval_results.get(f'paper_precision@{k}', 0):.4f}")
        report_lines.append("")

        # Dataset ë©”íŠ¸ë¦­
        report_lines.append(f"  ë°ì´í„°ì…‹ (Datasets):")
        report_lines.append(f"    - nDCG@{k}:     {eval_results.get(f'dataset_ndcg@{k}', 0):.4f}")
        report_lines.append(f"    - MRR@{k}:      {eval_results.get(f'dataset_mrr@{k}', 0):.4f}")
        report_lines.append(f"    - Recall@{k}:   {eval_results.get(f'dataset_recall@{k}', 0):.4f}")
        report_lines.append(f"    - Precision@{k}: {eval_results.get(f'dataset_precision@{k}', 0):.4f}")
        report_lines.append("")

    # í†µê³„ ì •ë³´
    report_lines.append("ğŸ“ˆ ì¶”ì²œ í†µê³„")
    report_lines.append("-" * 80)
    report_lines.append(f"  ì¶”ì²œëœ ë…¼ë¬¸ ìˆ˜:     {eval_results.get('num_predicted_papers', 0)}")
    report_lines.append(f"  ì¶”ì²œëœ ë°ì´í„°ì…‹ ìˆ˜: {eval_results.get('num_predicted_datasets', 0)}")
    report_lines.append(f"  ê´€ë ¨ ë…¼ë¬¸ ìˆ˜:       {eval_results.get('num_relevant_papers', 0)}")
    report_lines.append(f"  ê´€ë ¨ ë°ì´í„°ì…‹ ìˆ˜:   {eval_results.get('num_relevant_datasets', 0)}")
    report_lines.append("")
    report_lines.append("=" * 80)

    return "\n".join(report_lines)
