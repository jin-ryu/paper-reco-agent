"""
Qwen3-14B ì–¸ì–´ëª¨ë¸ ë˜í¼
í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ LLM ì¸í„°í˜ì´ìŠ¤
"""
import logging
import torch
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from src.config.settings import settings

logger = logging.getLogger(__name__)


class QwenModel:
    """
    Qwen3-14B ëª¨ë¸ ë˜í¼

    - 100+ ì–¸ì–´ ì§€ì› (ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“±)
    - 14.8B íŒŒë¼ë¯¸í„° (13.2B non-embedding)
    - ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    - í›„ë³´ ë¶„ì„ ë° ì¶”ì²œ ìƒì„±
    - ì¶”ì²œ ì´ìœ  ì‘ì„±
    """

    def __init__(self):
        self.model_name = settings.MODEL_NAME
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None

        self._load_model()

    def _load_model(self):
        """ëª¨ë¸ ë¡œë”© (FP16)"""
        try:
            logger.info(f"ğŸš€ Qwen ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=settings.MODEL_CACHE_DIR,
                trust_remote_code=True
            )

            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            load_kwargs = {
                "cache_dir": settings.MODEL_CACHE_DIR,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True
            }

            # FP16 ëª¨ë“œë¡œ ë¡œë“œ
            if self.device == "cuda":
                logger.info("   - FP16 ëª¨ë“œ (~28GB VRAM)")
                load_kwargs["torch_dtype"] = torch.float16
            else:
                logger.info("   - CPU ëª¨ë“œ (ëŠë¦¼)")

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **load_kwargs
            )

            # ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model = self.model.to(self.device)
            self.model.eval()

            logger.info(f"âœ… Qwen ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    async def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        try:
            if max_new_tokens is None:
                max_new_tokens = settings.MAX_TOKENS
            if temperature is None:
                temperature = settings.TEMPERATURE

            # Qwen3 ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (non-thinking ëª¨ë“œ ê°•ì œ)
            messages = [
                {"role": "system", "content": "You are a research recommendation assistant. You must output ONLY valid JSON format. Never use <think> tags or any explanations. Start your response directly with '{' character."},
                {"role": "user", "content": prompt}
            ]

            # í† í¬ë‚˜ì´ì§• (ì±„íŒ… í…œí”Œë¦¿ ì ìš©, thinking ëª¨ë“œ ë¹„í™œì„±í™”)
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False  # Qwen3 thinking ëª¨ë“œ ì™„ì „íˆ ë¹„í™œì„±í™”
            )

            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=32768  # Qwen3ëŠ” 32K í† í° ì§€ì› (í™•ì¥ ì‹œ 128K)
            ).to(self.device)

            # ìƒì„± (thinking mode ë¹„í™œì„±í™”)
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1  # ë°˜ë³µ ë°©ì§€
                )

            # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            return generated_text.strip()

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""


    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "dtype": "float16",
            "max_tokens": settings.MAX_TOKENS,
            "temperature": settings.TEMPERATURE,
            "parameters": "14.8B",
            "context_length": "32K (extendable to 128K)"
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            import gc

            # 1. Qwen ëª¨ë¸ ì‚­ì œ
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None

            # 2. í† í¬ë‚˜ì´ì € ì‚­ì œ
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None

            # 3. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            gc.collect()

            # 4. PyTorch CUDA ìºì‹œ ë¹„ìš°ê¸°
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # ëª¨ë“  CUDA ì‘ì—… ì™„ë£Œ ëŒ€ê¸°

            logger.info("âœ… ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
