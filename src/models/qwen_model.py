"""
Qwen3-14B ì–¸ì–´ëª¨ë¸ ë˜í¼
í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ LLM ì¸í„°í˜ì´ìŠ¤
"""
import logging
import torch
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from src.config.settings import settings

logger = logging.getLogger(__name__)


class QwenModel:
    """
    Qwen3-14B ëª¨ë¸ ë˜í¼

    - 100+ ì–¸ì–´ ì§€ì› (ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“±)
    - 14.8B íŒŒë¼ë¯¸í„° (13.2B non-embedding)
    - ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    - í›„ë³´ ë¶„ì„ ë° ì¶”ì²œ ìƒì„±
    - ì¶”ì²œ ì´ìœ  ì‘ì„±
    """

    def __init__(self):
        self.model_name = settings.MODEL_NAME
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None

        self._load_model()

    def _load_model(self):
        """ëª¨ë¸ ë¡œë”© (FP16)"""
        try:
            logger.info(f"ğŸš€ Qwen ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=settings.MODEL_CACHE_DIR,
                trust_remote_code=True
            )

            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            load_kwargs = {
                "cache_dir": settings.MODEL_CACHE_DIR,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True
            }

            # FP16 ëª¨ë“œë¡œ ë¡œë“œ
            if self.device == "cuda":
                logger.info("   - FP16 ëª¨ë“œ (~28GB VRAM)")
                load_kwargs["torch_dtype"] = torch.float16
            else:
                logger.info("   - CPU ëª¨ë“œ (ëŠë¦¼)")

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **load_kwargs
            )

            # ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model = self.model.to(self.device)
            self.model.eval()

            logger.info(f"âœ… Qwen ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    async def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        try:
            if max_new_tokens is None:
                max_new_tokens = settings.MAX_TOKENS
            if temperature is None:
                temperature = settings.TEMPERATURE

            # Qwen3 ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (non-thinking ëª¨ë“œ ê°•ì œ)
            messages = [
                {"role": "system", "content": "You are a research recommendation assistant. You must output ONLY valid JSON format. Never use <think> tags or any explanations. Start your response directly with '{' character."},
                {"role": "user", "content": prompt}
            ]

            # í† í¬ë‚˜ì´ì§• (ì±„íŒ… í…œí”Œë¦¿ ì ìš©, thinking ëª¨ë“œ ë¹„í™œì„±í™”)
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False  # Qwen3 thinking ëª¨ë“œ ì™„ì „íˆ ë¹„í™œì„±í™”
            )

            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=32768  # Qwen3ëŠ” 32K í† í° ì§€ì› (í™•ì¥ ì‹œ 128K)
            ).to(self.device)

            # ìƒì„± (thinking mode ë¹„í™œì„±í™”)
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1  # ë°˜ë³µ ë°©ì§€
                )

            # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            return generated_text.strip()

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    def create_korean_prompt(self, task_description: str, context: Dict[str, Any]) -> str:
        """
        í•œêµ­ì–´/ë‹¤êµ­ì–´ ì‘ì—…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (Qwen3 ìµœì í™”)

        Args:
            task_description: ì‘ì—… ì„¤ëª…
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        source_title = context.get('source_title', '')
        source_description = context.get('source_description', '')[:500]
        source_keywords = context.get('source_keywords', '')
        source_classification = context.get('source_classification', '')

        candidates = context.get('candidates', [])

        # í›„ë³´ ì •ë³´ í¬ë§·íŒ… (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        candidates_text = ""
        for i, candidate in enumerate(candidates[:15], 1):
            cand_type = candidate.get('type', 'unknown')
            title = candidate.get('title', '')
            desc = candidate.get('description', '')[:150]

            # ìœ ì‚¬ë„ ì ìˆ˜ ìƒì„¸ ì •ë³´
            semantic_score = candidate.get('semantic_score', 0.0)
            lexical_score = candidate.get('lexical_score', 0.0)
            final_score = candidate.get('final_score', 0.0)
            common_keywords = candidate.get('common_keywords', [])[:5]

            keywords = ', '.join(candidate.get('keywords', []))[:150]

            candidates_text += f"\n[{i}] ({cand_type}) {title}\n"
            candidates_text += f"   Description: {desc}...\n"
            candidates_text += f"   Similarity: Semantic {semantic_score:.2f} | Lexical {lexical_score:.2f} | Final {final_score:.2f}\n"
            if common_keywords:
                candidates_text += f"   Common terms: {', '.join(common_keywords)}\n"
            if keywords:
                candidates_text += f"   Keywords: {keywords}\n"

        prompt = f"""# Task
You are a research data and paper recommendation expert.

**Important**: E5 embedding model has already calculated similarity scores. Your role:
1. Analyze similarity scores and select most relevant candidates
2. Write specific and logical reasons for each recommendation
3. Determine recommendation level (ê°•ì¶”/ì¶”ì²œ/ì°¸ê³ )

{task_description}

## Recommendation Level Criteria:
- ê°•ì¶” (Strong): Final similarity â‰¥ 0.75, both semantic+lexical high
- ì¶”ì²œ (Recommend): Final similarity â‰¥ 0.60, semantic or lexical high
- ì°¸ê³  (Reference): Final similarity â‰¥ 0.45, partial relevance

## Source Dataset:
Title: {source_title}
Description: {source_description}...
Keywords: {source_keywords}
Classification: {source_classification}

## Candidates (Filtered by E5 embedding):
{candidates_text}

## Output Format:
Output ONLY valid JSON. No explanations, comments, or examples.

{{
  "recommendations": [
    {{
      "candidate_number": 1,
      "reason": "High semantic similarity with common keywords",
      "level": "ê°•ì¶”"
    }},
    {{
      "candidate_number": 2,
      "reason": "Related research field",
      "level": "ì¶”ì²œ"
    }}
  ]
}}

Rules:
- Output JSON only, start with '{{' character
- candidate_number: Number from candidate list above (1-15)
- reason: One concise sentence explaining why this is relevant
- level: "ê°•ì¶”" (â‰¥0.75) or "ì¶”ì²œ" (â‰¥0.60) or "ì°¸ê³ " (â‰¥0.45)
- DO NOT include title, type, or score - only candidate_number, reason, level
"""
        return prompt

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "dtype": "float16",
            "max_tokens": settings.MAX_TOKENS,
            "temperature": settings.TEMPERATURE,
            "parameters": "14.8B",
            "context_length": "32K (extendable to 128K)"
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model:
                del self.model
            if self.tokenizer:
                del self.tokenizer

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("âœ… ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
