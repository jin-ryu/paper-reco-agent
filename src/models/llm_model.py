"""
ë²”ìš© ì–¸ì–´ëª¨ë¸ ë˜í¼
í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ LLM ì¸í„°í˜ì´ìŠ¤
ì§€ì› ëª¨ë¸: Qwen3-14B, Gemma2-9B ë“±
"""
import logging
import torch
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from src.config.settings import settings

logger = logging.getLogger(__name__)


class LLMModel:
    """
    ë²”ìš© ì–¸ì–´ëª¨ë¸ ë˜í¼ (MODEL_NAME í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë¸ ì„ íƒ)

    ì§€ì› ëª¨ë¸:
    - Qwen/Qwen3-14B (14.8B íŒŒë¼ë¯¸í„°, 32K context)
    - google/gemma-2-9b-it (9B íŒŒë¼ë¯¸í„°, 8K context)
    - ê¸°íƒ€ Hugging Faceì˜ causal LM ëª¨ë¸

    ê¸°ëŠ¥:
    - ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    - í›„ë³´ ë¶„ì„ ë° ì¶”ì²œ ìƒì„±
    - ì¶”ì²œ ì´ìœ  ì‘ì„±
    """

    def __init__(self):
        self.model_name = settings.MODEL_NAME
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None

        # ëª¨ë¸ ì¢…ë¥˜ ê°ì§€
        self.is_qwen = "qwen" in self.model_name.lower()
        self.is_gemma = "gemma" in self.model_name.lower()

        # ëª¨ë¸ë³„ ì„¤ì •
        self._setup_model_config()

        self._load_model()

    def _setup_model_config(self):
        """ëª¨ë¸ë³„ ì„¤ì •"""
        if self.is_qwen:
            self.model_type = "Qwen"
            self.max_context_length = 32768
            self.supports_thinking = True
        elif self.is_gemma:
            self.model_type = "Gemma"
            self.max_context_length = 8192
            self.supports_thinking = False
        else:
            self.model_type = "Generic"
            self.max_context_length = 4096
            self.supports_thinking = False

    def _load_model(self):
        """ëª¨ë¸ ë¡œë”© (FP16)"""
        try:
            logger.info(f"ğŸš€ {self.model_type} ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer_kwargs = {
                "cache_dir": settings.MODEL_CACHE_DIR,
                "trust_remote_code": True
            }
            if settings.HF_TOKEN:
                tokenizer_kwargs["token"] = settings.HF_TOKEN

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                **tokenizer_kwargs
            )

            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            load_kwargs = {
                "cache_dir": settings.MODEL_CACHE_DIR,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True
            }
            if settings.HF_TOKEN:
                load_kwargs["token"] = settings.HF_TOKEN

            # FP16 ëª¨ë“œë¡œ ë¡œë“œ
            if self.device == "cuda":
                logger.info("   - FP16 ëª¨ë“œ")
                load_kwargs["torch_dtype"] = torch.float16
            else:
                logger.info("   - CPU ëª¨ë“œ (ëŠë¦¼)")

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **load_kwargs
            )

            # ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model = self.model.to(self.device)
            self.model.eval()

            logger.info(f"âœ… {self.model_type} ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    async def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        try:
            if max_new_tokens is None:
                max_new_tokens = settings.MAX_TOKENS
            if temperature is None:
                temperature = settings.TEMPERATURE

            # ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            messages = [
                {"role": "system", "content": "You are a research recommendation assistant. You must output ONLY valid JSON format. Never use <think> tags or any explanations. Start your response directly with '{' character."},
                {"role": "user", "content": prompt}
            ]

            # í† í¬ë‚˜ì´ì§• (ëª¨ë¸ë³„ ì²˜ë¦¬)
            if self.is_qwen:
                # Qwen3: thinking ëª¨ë“œ ë¹„í™œì„±í™”
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=False  # Qwen3 thinking ëª¨ë“œ ì™„ì „íˆ ë¹„í™œì„±í™”
                )
            else:
                # Gemma ë° ê¸°íƒ€ ëª¨ë¸: ì¼ë°˜ ì±„íŒ… í…œí”Œë¦¿
                try:
                    text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                except Exception:
                    # ì±„íŒ… í…œí”Œë¦¿ ì§€ì› ì•ˆí•˜ëŠ” ê²½ìš° ì§ì ‘ í¬ë§·íŒ…
                    text = f"System: {messages[0]['content']}\n\nUser: {messages[1]['content']}\n\nAssistant:"

            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_context_length
            ).to(self.device)

            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id if self.tokenizer.eos_token_id else self.tokenizer.pad_token_id,
                    repetition_penalty=1.1  # ë°˜ë³µ ë°©ì§€
                )

            # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            return generated_text.strip()

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""


    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
        if self.is_qwen and "14b" in self.model_name.lower():
            params = "14.8B"
        elif self.is_gemma and "9b" in self.model_name.lower():
            params = "9B"
        elif self.is_gemma and "2b" in self.model_name.lower():
            params = "2B"
        else:
            params = "Unknown"

        return {
            "model_name": self.model_name,
            "model_type": self.model_type,
            "device": self.device,
            "dtype": "float16",
            "max_tokens": settings.MAX_TOKENS,
            "temperature": settings.TEMPERATURE,
            "parameters": params,
            "context_length": f"{self.max_context_length // 1024}K"
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            import gc

            # 1. ëª¨ë¸ ì‚­ì œ
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None

            # 2. í† í¬ë‚˜ì´ì € ì‚­ì œ
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None

            # 3. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            gc.collect()

            # 4. PyTorch CUDA ìºì‹œ ë¹„ìš°ê¸°
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # ëª¨ë“  CUDA ì‘ì—… ì™„ë£Œ ëŒ€ê¸°

            logger.info("âœ… ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
