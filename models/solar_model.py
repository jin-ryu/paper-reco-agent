"""
SOLAR-10.7B ì–¸ì–´ëª¨ë¸ ë˜í¼
í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ LLM ì¸í„°í˜ì´ìŠ¤
"""
import logging
import torch
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from config.settings import settings

# accelerateê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
try:
    import accelerate
    ACCELERATE_AVAILABLE = True
except ImportError:
    ACCELERATE_AVAILABLE = False

# bitsandbytesê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (INT4/INT8 ì–‘ìí™”ìš©)
try:
    import bitsandbytes
    BITSANDBYTES_AVAILABLE = True
except Exception:
    BITSANDBYTES_AVAILABLE = False

logger = logging.getLogger(__name__)


class SolarModel:
    """
    SOLAR-10.7B Instruct ëª¨ë¸ ë˜í¼

    - ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    - í›„ë³´ ë¶„ì„ ë° ì¶”ì²œ ìƒì„±
    - ì¶”ì²œ ì´ìœ  ì‘ì„±
    """

    def __init__(self):
        self.model_name = settings.MODEL_NAME
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None

        self._load_model()

    def _load_model(self):
        """ëª¨ë¸ ë¡œë”© (ì–‘ìí™” ì§€ì›)"""
        try:
            logger.info(f"ğŸš€ SOLAR ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
            logger.info(f"   - ì–‘ìí™”: {settings.QUANTIZATION}")

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=settings.MODEL_CACHE_DIR,
                trust_remote_code=True
            )

            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            load_kwargs = {
                "cache_dir": settings.MODEL_CACHE_DIR,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True
            }

            # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
            if settings.QUANTIZATION in ["int8", "int4"] and self.device == "cuda":
                # INT8/INT4ëŠ” bitsandbytes í•„ìš”
                if not BITSANDBYTES_AVAILABLE:
                    logger.warning("âš ï¸  bitsandbytesê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ CUDA ë¬¸ì œ ë°œìƒ")
                    logger.warning(f"   {settings.QUANTIZATION} ì–‘ìí™” ë¶ˆê°€ - FP16ìœ¼ë¡œ ëŒ€ì²´")
                    logger.info("   - FP16 ëª¨ë“œë¡œ ì „í™˜ (~21GB VRAM)")
                    load_kwargs["torch_dtype"] = torch.float16
                    if ACCELERATE_AVAILABLE:
                        load_kwargs["device_map"] = "auto"
                elif settings.QUANTIZATION == "int8":
                    logger.info("   - INT8 ì–‘ìí™” í™œì„±í™” (~11GB VRAM)")
                    load_kwargs["load_in_8bit"] = True
                    load_kwargs["device_map"] = "auto"
                elif settings.QUANTIZATION == "int4":
                    logger.info("   - INT4 ì–‘ìí™” í™œì„±í™” (~6GB VRAM)")
                    load_kwargs["load_in_4bit"] = True
                    load_kwargs["device_map"] = "auto"
            elif self.device == "cuda":
                # device_map ì—†ì´ ë‹¨ìˆœ ë¡œë“œ (accelerate ì´ìŠˆ íšŒí”¼)
                logger.info("   - FP16 ëª¨ë“œ (~21GB VRAM, device_map ì—†ìŒ)")
                load_kwargs["torch_dtype"] = torch.float16
            else:
                logger.info("   - CPU ëª¨ë“œ (ëŠë¦¼)")

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **load_kwargs
            )

            # device_map ì—†ì„ ë•Œë§Œ ëª…ì‹œì ìœ¼ë¡œ ì´ë™
            if "device_map" not in load_kwargs:
                self.model = self.model.to(self.device)

            self.model.eval()

            logger.info(f"âœ… SOLAR ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    async def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        try:
            if max_new_tokens is None:
                max_new_tokens = settings.MAX_TOKENS
            if temperature is None:
                temperature = settings.TEMPERATURE

            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)

            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            return generated_text.strip()

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    def create_korean_prompt(self, task_description: str, context: Dict[str, Any]) -> str:
        """
        í•œêµ­ì–´ ì‘ì—…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (SOLAR ìµœì í™”)

        Args:
            task_description: ì‘ì—… ì„¤ëª…
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        source_title = context.get('source_title', '')
        source_description = context.get('source_description', '')[:500]
        source_keywords = context.get('source_keywords', '')
        source_classification = context.get('source_classification', '')

        candidates = context.get('candidates', [])

        # í›„ë³´ ì •ë³´ í¬ë§·íŒ… (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        candidates_text = ""
        for i, candidate in enumerate(candidates[:15], 1):
            cand_type = candidate.get('type', 'unknown')
            title = candidate.get('title', '')
            desc = candidate.get('description', '')[:150]

            # ìœ ì‚¬ë„ ì ìˆ˜ ìƒì„¸ ì •ë³´
            semantic_score = candidate.get('semantic_score', 0.0)
            lexical_score = candidate.get('lexical_score', 0.0)
            final_score = candidate.get('final_score', 0.0)
            common_keywords = candidate.get('common_keywords', [])[:5]

            keywords = ', '.join(candidate.get('keywords', []))[:150]

            candidates_text += f"\n[{i}] ({cand_type}) {title}\n"
            candidates_text += f"   ì„¤ëª…: {desc}...\n"
            candidates_text += f"   ìœ ì‚¬ë„: ì˜ë¯¸ì  {semantic_score:.2f} | ì–´íœ˜ì  {lexical_score:.2f} | ìµœì¢… {final_score:.2f}\n"
            if common_keywords:
                candidates_text += f"   ê³µí†µ ìš©ì–´: {', '.join(common_keywords)}\n"
            if keywords:
                candidates_text += f"   í‚¤ì›Œë“œ: {keywords}\n"

        prompt = f"""### Instruction:
ë‹¹ì‹ ì€ ì—°êµ¬ ë°ì´í„°ì™€ ë…¼ë¬¸ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

**ì¤‘ìš”**: ì„ë² ë”© ëª¨ë¸(E5)ì´ ì´ë¯¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—­í• ì€:
1. ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í›„ë³´ ì„ ë³„
2. ê° ì¶”ì²œì— ëŒ€í•´ **êµ¬ì²´ì ì´ê³  ë…¼ë¦¬ì ì¸ ì´ìœ ** ì‘ì„±
3. ì¶”ì²œ ë ˆë²¨ ê²°ì • (ê°•ì¶”/ì¶”ì²œ/ì°¸ê³ )

{task_description}

### ì¶”ì²œ ë ˆë²¨ ê¸°ì¤€:
- **ê°•ì¶”**: ìµœì¢… ìœ ì‚¬ë„ â‰¥ 0.75, ì˜ë¯¸ì +ì–´íœ˜ì  ëª¨ë‘ ë†’ìŒ
- **ì¶”ì²œ**: ìµœì¢… ìœ ì‚¬ë„ â‰¥ 0.60, ì˜ë¯¸ì  ë˜ëŠ” ì–´íœ˜ì  ë†’ìŒ
- **ì°¸ê³ **: ìµœì¢… ìœ ì‚¬ë„ â‰¥ 0.45, ë¶€ë¶„ì  ì—°ê´€ì„±

### Source Dataset:
ì œëª©: {source_title}
ì„¤ëª…: {source_description}...
í‚¤ì›Œë“œ: {source_keywords}
ë¶„ë¥˜: {source_classification}

### Candidates (E5 ì„ë² ë”©ìœ¼ë¡œ í•„í„°ë§ë¨):
{candidates_text}

### Response Format:
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. **ì¶”ê°€ ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥**í•˜ì„¸ìš”.

```json
{{
  "recommendations": [
    {{
      "candidate_number": 1,
      "title": "í›„ë³´ ì œëª© ê·¸ëŒ€ë¡œ",
      "type": "paper ë˜ëŠ” dataset",
      "score": 0.85,
      "reason": "ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.85ë¡œ ë§¤ìš° ë†’ìŒ; ê³µí†µ í‚¤ì›Œë“œ 'A', 'B', 'C'ë¡œ ì£¼ì œ ì¼ì¹˜; ì–´íœ˜ì  ë§¤ì¹­ë„ ë†’ì•„ í•µì‹¬ ìš©ì–´ ê³µìœ ",
      "level": "ê°•ì¶”"
    }}
  ]
}}
```

**ì¤‘ìš”**:
- reasonì€ 2-3ë¬¸ì¥ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
- ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê·¼ê±°ë¡œ í™œìš©
- ê³µí†µ ìš©ì–´/í‚¤ì›Œë“œë¥¼ ì–¸ê¸‰
"""
        return prompt

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "quantization": settings.QUANTIZATION,
            "max_tokens": settings.MAX_TOKENS,
            "temperature": settings.TEMPERATURE,
            "parameters": "10.7B"
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model:
                del self.model
            if self.tokenizer:
                del self.tokenizer

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("âœ… ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
